# Open Deep Research

Open Deep Research is an experimental, fully open-source research assistant that automates deep research and produces comprehensive reports on any topic. It uses a [LangGraph](https://langchain-ai.github.io/langgraph/) workflow to structure the research process. You can customize the entire research and writing process with specific models, prompts, report structure, and search tools.

### ğŸš€ Workflow

![open-deep-research-overview](https://github.com/user-attachments/assets/a171660d-b735-4587-ab2f-cd771f773756)

### ğŸƒ Quickstart

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/langchain-ai/open_deep_research.git
    cd open_deep_research
    ```

2.  **Set up your environment:**

    Copy the example environment file and edit it to add your API keys for language models and search tools.
    ```bash
    cp .env.example .env
    ```

3.  **Install dependencies:**

    *   **Using `uv` (æ¨èæ–¹å¼):**
        ```bash
        # å®‰è£… uvï¼ˆå¦‚æœå°šæœªå®‰è£…ï¼‰
        curl -LsSf https://astral.sh/uv/install.sh | sh

        # åˆ›å»ºè™šæ‹Ÿç¯å¢ƒå¹¶æŒ‰ç…§ uv.lock å®‰è£…ç²¾ç¡®ç‰ˆæœ¬çš„ä¾èµ–
        uv sync
        ```

    *   **Using `pip` (ä¼ ç»Ÿæ–¹å¼):**
        ```bash
        # Create and activate a virtual environment
        python -m venv .venv
        source .venv/bin/activate  # On Windows use `.venv\Scripts\activate`

        # Install dependencies
        pip install -e ".[dev]"
        ```

4.  **Launch the UI:**
    
    *   **Using `uv` (æ¨è):**
        ```bash
        # ä½¿ç”¨ uv è¿è¡Œï¼Œè‡ªåŠ¨ä½¿ç”¨è™šæ‹Ÿç¯å¢ƒå’Œé”å®šç‰ˆæœ¬
        uv run python start.py
        ```
        
    *   **Using activated virtual environment:**
        ```bash
        # æ‰‹åŠ¨æ¿€æ´»è™šæ‹Ÿç¯å¢ƒåè¿è¡Œ
        source .venv/bin/activate  # On Windows use `.venv\Scripts\activate`
        python start.py
        ```
    
    This will open the user interface in your browser.

5.  **è¿è¡Œ LangGraph å¹³å°ï¼ˆå¯é€‰ï¼‰:**

    é™¤äº† Streamlit UIï¼Œä½ ä¹Ÿå¯ä»¥ä½¿ç”¨ LangGraph å¹³å°è¿›è¡Œå¼€å‘å’Œæµ‹è¯•ï¼š

    ```bash
    # ä½¿ç”¨ uv è¿è¡Œ LangGraph å¼€å‘æœåŠ¡å™¨
    uv run uvx --refresh --from "langgraph-cli[inmem]" --with-editable . --python 3.11 langgraph dev --allow-blocking
    ```
    
    è¿™å°†å¯åŠ¨ LangGraph å¼€å‘æœåŠ¡å™¨ï¼Œæä¾›å›¾å½¢åŒ–ç•Œé¢æ¥è°ƒè¯•å’Œæµ‹è¯•å·¥ä½œæµã€‚

### ğŸ’¡ ä¾èµ–ç®¡ç†è¯´æ˜

é¡¹ç›®ä½¿ç”¨ `uv.lock` æ–‡ä»¶ç¡®ä¿æ‰€æœ‰ç¯å¢ƒä½¿ç”¨å®Œå…¨ç›¸åŒçš„ä¾èµ–ç‰ˆæœ¬ï¼š

- **å¼€å‘æ—¶**: ä½¿ç”¨ `uv run python start.py` ç¡®ä¿ä½¿ç”¨é”å®šç‰ˆæœ¬
- **æ·»åŠ ä¾èµ–**: `uv add package-name` 
- **æ›´æ–°ä¾èµ–**: `uv sync`
- **éƒ¨ç½²æ—¶**: Docker å’Œ Railway ä¼šè‡ªåŠ¨ä½¿ç”¨ `uv.lock` ä¸­çš„ç²¾ç¡®ç‰ˆæœ¬

è¿™è§£å†³äº†"åœ¨æˆ‘ç”µè„‘ä¸Šèƒ½è¿è¡Œ"çš„ç‰ˆæœ¬ä¸ä¸€è‡´é—®é¢˜ã€‚

---

### ğŸš¢ éƒ¨ç½² Deployment

#### 1. Docker æœ¬åœ°/æœåŠ¡å™¨éƒ¨ç½²

ä»“åº“å·²æä¾›åŸºäº `uv` çš„ `Dockerfile`ï¼Œç¡®ä¿ç‰ˆæœ¬ä¸€è‡´æ€§ï¼š
```bash
# ç¡®ä¿æœ¬åœ°æœ‰æœ€æ–°çš„ uv.lock æ–‡ä»¶
uv sync

# æ„å»ºé•œåƒï¼ˆä½¿ç”¨ uv.lock ä¸­çš„ç²¾ç¡®ç‰ˆæœ¬ï¼‰
docker build -t open-deep-research:latest .

# è¿è¡Œå®¹å™¨ï¼Œæ˜ å°„é»˜è®¤çš„ 8501 ç«¯å£
# è¯·ç”¨ -e ä¼ é€’æ¨¡å‹ / æœç´¢ API KEYï¼Œæˆ–æŒ‚è½½è‡ªå®šä¹‰ .env æ–‡ä»¶

docker run -it --rm -p 8501:8501 \
  -e OPENAI_API_KEY=your_key \
  -e TAVILY_API_KEY=your_key \
  open-deep-research:latest
```
å¯åŠ¨åè®¿é—® `http://localhost:8501` å³å¯ã€‚

**ä¼˜åŠ¿**: ä½¿ç”¨ `uv.lock` ç¡®ä¿å®¹å™¨å†…ä¾èµ–ç‰ˆæœ¬ä¸æœ¬åœ°å¼€å‘ç¯å¢ƒå®Œå…¨ä¸€è‡´ã€‚

#### 2. Railway äº‘éƒ¨ç½²

å·²æä¾› `railway.json` é…ç½®ä¸è¾…åŠ©è„šæœ¬ `deploy.sh`ï¼š
```bash
# ç¡®ä¿ä¾èµ–é”å®šæ–‡ä»¶æœ€æ–°
uv sync

# å®‰è£…ä¾èµ–ï¼ˆéœ€å…ˆå®‰è£… Node & npmï¼‰
npm install -g @railway/cli

# ä¸€é”®åˆ›å»ºå¹¶éƒ¨ç½²ï¼ˆä¼šæç¤ºè¾“å…¥ API KEYï¼‰
./deploy.sh
```
è„šæœ¬ä¼šè‡ªåŠ¨ï¼š
1. æ£€æŸ¥å¿…è¦æ–‡ä»¶ï¼ˆ`uv.lock`ã€`pyproject.toml`ã€`Dockerfile`ï¼‰
2. æ£€æŸ¥/å®‰è£… Railway CLI å¹¶ç™»å½•
3. åˆ›å»ºåä¸º `open-deep-research` çš„é¡¹ç›®
4. æ ¹æ®æç¤ºå†™å…¥ç¯å¢ƒå˜é‡
5. ä½¿ç”¨åŸºäº `uv` çš„ `Dockerfile` æ„å»ºå¹¶éƒ¨ç½²

**é‡è¦**: ç¡®ä¿ `uv.lock` æ–‡ä»¶å·²æäº¤åˆ° Gitï¼Œè¿™æ ·äº‘ç«¯æ„å»ºä¼šä½¿ç”¨ä¸æœ¬åœ°å®Œå…¨ç›¸åŒçš„ä¾èµ–ç‰ˆæœ¬ã€‚

éƒ¨ç½²å®Œæˆåï¼Œå¯åœ¨ Railway Dashboard æŸ¥çœ‹ URL ä¸æ—¥å¿—ã€‚

#### 3. å…¶ä»–å¹³å°
- **Streamlit Community Cloud / Hugging Face Spaces**ï¼šå…¥å£æ–‡ä»¶å‡ä¸º `start.py`ï¼›å°† `.env` ä¸­çš„å˜é‡å¡«å…¥å„å¹³å°çš„ Secrets å³å¯ã€‚
- **Vercel / Netlify**ï¼šå»ºè®®å…ˆç”¨ Docker å®¹å™¨æˆ– Cloud Run éƒ¨ç½²åå†åšåå‘ä»£ç†ã€‚

> âš ï¸ è¯·å‹¿å°†åŒ…å«æ•æ„Ÿä¿¡æ¯çš„ `.env` æ–‡ä»¶æäº¤åˆ°å…¬å…±ä»“åº“ï¼ŒåŠ¡å¿…é€šè¿‡å¹³å° Secrets æˆ–å‘½ä»¤è¡Œ `-e` å‚æ•°æ³¨å…¥ã€‚

### ğŸ”§ æ•…éšœæ’é™¤

#### LangSmith Playground å…¼å®¹æ€§é—®é¢˜

å¦‚æœåœ¨ LangSmith playground ä¸­é‡åˆ° `TypeError: AsyncCompletions.parse() got an unexpected keyword argument 'output_version'` é”™è¯¯ï¼š

**åŸå› **: OpenAI Python åŒ…ç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜ã€‚é¡¹ç›®ä½¿ç”¨äº†è¾ƒæ–°çš„ OpenAI ç‰ˆæœ¬ï¼Œè€Œ LangSmith playground å¯èƒ½ä½¿ç”¨äº†è¾ƒæ—§çš„å®¢æˆ·ç«¯ç‰ˆæœ¬ã€‚

**è§£å†³æ–¹æ¡ˆ**: 

**æ–¹æ³•ä¸€ï¼šä½¿ç”¨ LangChain Hubï¼ˆæ¨èï¼‰**
```python
# å°†æç¤ºè¯ä¿å­˜åˆ° LangChain Hubï¼Œç„¶ååœ¨ä»£ç ä¸­ä½¿ç”¨
from langchain import hub

# ä¿å­˜æç¤ºè¯åˆ° Hub
prompt = hub.push("your-username/prompt-name", your_prompt)

# åœ¨ä»£ç ä¸­ä½¿ç”¨
prompt = hub.pull("your-username/prompt-name")
```

**æ–¹æ³•äºŒï¼šæœ¬åœ°æµ‹è¯•ç¯å¢ƒ**
åˆ›å»ºå•ç‹¬çš„æµ‹è¯•ç¯å¢ƒï¼š
```bash
# åˆ›å»ºå…¼å®¹ç¯å¢ƒï¼ˆä»…ç”¨äº LangSmith æµ‹è¯•ï¼‰
python -m venv langsmith_test_env
source langsmith_test_env/bin/activate  # æˆ– langsmith_test_env\Scripts\activate (Windows)

# å®‰è£…å…¼å®¹ç‰ˆæœ¬
pip install openai==1.40.6 langchain-openai==0.3.6 langsmith>=0.3.37

# æµ‹è¯•ä½ çš„ä»£ç 
python your_test_script.py
```

#### Railway äº‘ç«¯éƒ¨ç½²æ•ˆæœå·®å¼‚

å¦‚æœäº‘ç«¯æ•ˆæœä¸æœ¬åœ°ä¸åŒï¼š

**ğŸ”§ ä½¿ç”¨éƒ¨ç½²æ£€æŸ¥è„šæœ¬**
```bash
# è¿è¡Œç¯å¢ƒæ£€æŸ¥è„šæœ¬
uv run python deploy_check.py
```
è¿™ä¸ªè„šæœ¬ä¼šæ£€æŸ¥ï¼š
- Pythonç‰ˆæœ¬å’Œå¹³å°ä¿¡æ¯
- ç¯å¢ƒå˜é‡é…ç½®
- å…³é”®ä¾èµ–ç‰ˆæœ¬
- OpenAI APIè¿æ¥çŠ¶æ€
- Streamlité…ç½®

**ğŸ“¦ ç‰ˆæœ¬ä¸€è‡´æ€§æ£€æŸ¥**
```bash
# æ£€æŸ¥æœ¬åœ°å’Œäº‘ç«¯æ˜¯å¦ä½¿ç”¨ç›¸åŒçš„ä¾èµ–ç‰ˆæœ¬
uv sync --frozen  # ç¡®ä¿ä½¿ç”¨ uv.lock ä¸­çš„ç²¾ç¡®ç‰ˆæœ¬
git status        # ç¡®è®¤ uv.lock å·²æäº¤
```

1. **æ£€æŸ¥ç¯å¢ƒå˜é‡**
   åœ¨ Railway dashboard ä¸­ç¡®è®¤æ‰€æœ‰å¿…éœ€çš„ç¯å¢ƒå˜é‡å·²æ­£ç¡®è®¾ç½®ï¼š
   ```
   OPENAI_API_KEY=your_key_here
   ```

2. **ç¡®ä¿ä¾èµ–ç‰ˆæœ¬ä¸€è‡´**
   ä½¿ç”¨ `uv.lock` æ–‡ä»¶ç¡®ä¿ç‰ˆæœ¬ä¸€è‡´æ€§ï¼š
   ```bash
   # æ›´æ–° uv.lockï¼ˆå¦‚æœ‰ä¾èµ–æ›´æ”¹ï¼‰
   uv sync
   
   # æäº¤é”å®šæ–‡ä»¶åˆ° Git
   git add uv.lock && git commit -m "Update dependencies lock" && git push
   ```

3. **æ£€æŸ¥æ„å»ºæ—¥å¿—**
   åœ¨ Railway dashboard æŸ¥çœ‹æ„å»ºæ—¥å¿—ï¼Œç¡®è®¤ï¼š
   - uv å®‰è£…æˆåŠŸ
   - `uv sync --frozen` æ‰§è¡ŒæˆåŠŸ
   - ä¾èµ–ç‰ˆæœ¬ä¸æœ¬åœ°ä¸€è‡´
   - ç¯å¢ƒå˜é‡è¯»å–æ­£ç¡®

4. **å¼ºåˆ¶é‡æ–°éƒ¨ç½²**
   ```bash
   # è§¦å‘é‡æ–°éƒ¨ç½²
   git commit --allow-empty -m "Force redeploy" && git push
   ```

5. **æœ¬åœ° Docker æµ‹è¯•**
   ```bash
   # ä½¿ç”¨ä¸äº‘ç«¯ç›¸åŒçš„æ„å»ºæµç¨‹æµ‹è¯•
   docker build -t test-build .
   docker run -p 8501:8501 -e OPENAI_API_KEY=your_key test-build
   ```

### ğŸ“ How to Use

1.  **Enter a topic** for your research in the input box.
2.  The system will generate a **research plan**. You can review it.
3.  If you are satisfied with the plan, click **"Accept"** to proceed. If you want to modify it, provide your feedback in the text box and click **"Regenerate"**.
4.  Once the plan is accepted, the workflow will execute the research and generate a **comprehensive report** in Markdown format.

### ğŸ› ï¸ Search Tools

You can configure the workflow to use various search tools. Set your preferences and API keys in the `.env` file.

*   [Tavily API](https://tavily.com/)
*   [Perplexity API](https://www.perplexity.ai/)
*   [Exa API](https://exa.ai/)
*   [ArXiv](https://arxiv.org/)
*   [PubMed](https://pubmed.ncbi.nlm.nih.gov/)
*   [Linkup API](https://www.linkup.so/)
*   [DuckDuckGo API](https://duckduckgo.com/)
*   [Google Search API](https://developers.google.com/custom-search/v1/introduction)
*   [Microsoft Azure AI Search](https://azure.microsoft.com/en-us/products/ai-services/ai-search)

### âš™ï¸ Advanced Configuration

#### Customizing the Workflow

You can customize the research workflow through several parameters:

- `report_structure`: Define a custom structure for your report.
- `number_of_queries`: Number of search queries to generate per section (default: 2).
- `max_search_depth`: Maximum number of reflection and search iterations (default: 2).
- `planner_provider` / `planner_model`: The model used for planning the report.
- `writer_provider` / `writer_model`: The model used for writing the report sections.
- `search_api`: The search API to use.

#### Search API Configuration

Some search APIs support additional configuration parameters.

- **Exa**: `max_characters`, `num_results`, `include_domains`, `exclude_domains`, `subpages`
- **ArXiv**: `load_max_docs`, `get_full_documents`, `load_all_available_meta`
- **PubMed**: `top_k_results`, `email`, `api_key`, `doc_content_chars_max`
- **Linkup**: `depth`

Example with Exa configuration:
```python
thread = {"configurable": {"thread_id": str(uuid.uuid4()),
                           "search_api": "exa",
                           "search_api_config": {
                               "num_results": 5,
                               "include_domains": ["nature.com", "sciencedirect.com"]
                           },
                           # Other configuration...
                           }}
```

### ğŸ¤– Model Considerations

1.  **Model Support**: You can use models supported by [the `init_chat_model()` API](https://python.langchain.com/docs/how_to/chat_models_universal_init/).
2.  **Structured Outputs**: The workflow's planner and writer models must support structured outputs/function calling. Check your model provider's documentation. Models from OpenAI, Anthropic, and Google generally work well.
3.  **Local Models**: For guidance on using local models with Ollama, see [this issue](https://github.com/langchain-ai/open_deep_research/issues/65#issuecomment-2743586318).

### ğŸ§ª Evaluation

The project includes a `pytest`-based evaluation system to assess report quality.

- **Run evaluation:**
  ```bash
  # Test with specific models
  python tests/run_test.py --planner-model "openai:gpt-4o" --writer-model "openai:gpt-4o" --eval-model "openai:gpt-4o"
  ```
- **Key Files:**
  - `tests/run_test.py`: Main test runner.
  - `tests/test_report_quality.py`: Core test implementation.
  - `tests/conftest.py`: Pytest configuration.

### ğŸ¤ Contributing

Contributions are welcome! Please feel free to open an issue or submit a pull request.

### ğŸ“„ License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.
- Gate-keeping before production deployments

**Use LangSmith System for:**
- Comprehensive model evaluation across datasets
- Research and analysis of system performance
- Detailed performance profiling and benchmarking
- Comparative studies between different configurations
- Production monitoring and quality assurance

Both evaluation systems complement each other and provide comprehensive coverage for different use cases and development stages.

## UX

### Local deployment

Follow the [quickstart](#-quickstart) to start LangGraph server locally.

### Hosted deployment
 
You can easily deploy to [LangGraph Platform](https://langchain-ai.github.io/langgraph/concepts/#deployment-options). 
